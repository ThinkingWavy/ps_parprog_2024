---
title: "PS-Parallele Programmierung Exc 4"
author: Marco Fr√∂hlich, Johanna Backer and Camillo Zanolin
output: pdf_document
date: ""
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)

```

# Exercise 1
All measurements where done on the LCC3 and the result of the mean of 9 executions.

## Task 1: Benchmark pthread version
```{r pthread Benchmark, echo=FALSE}
data<-data.frame(
  num_threads=c(1,2,4,6,12),
  measurements = c(10.943, 5.493, 2.813, 1.916, 0.961)
)

q <- ggplot(data, aes(x=num_threads, y=measurements)) +
  ggtitle("Benchmark pthread version")+
  geom_line() +
  geom_point() +
  xlab("number threads") +
  ylab("Execution time in seconds")+
  scale_x_continuous(limits = c(1,12), breaks = c(1,2,4,6,12))+
  geom_text(aes(label=measurements), nudge_y = -0.5)
show(q)
```

### Observation:
More threads = less execution time

## Task 4: Benchmarking different OpenMP versions with OpenMP wtime
```{r OMP Benchmark wtime, echo=FALSE}
data<-data.frame(
  legend = factor(rep(c("critcal", "atomic", "reduction"), each=5)),
  num_threads=rep(c(1,2,4,6,12), times=3),
  measurements = c(14.185, 41.411, 108.166, 148.775, 179.67, # critical
                    11.29, 14.452, 22.791, 35.064, 22.91,# atomic
                    8.419, 4.209, 2.130, 1.469, 0.808 # reduction
                   ) 
)

r <- ggplot(data, aes(x=num_threads, y=measurements, color= legend, group=legend)) +
  ggtitle("Benchmark OMP versions, OpenMp wtime")+
  geom_line() +
  geom_point() +
  xlab("number threads") +
  ylab("Execution time in seconds")+
  scale_x_continuous(limits = c(1,12), breaks = c(1,2,4,6,12))

show(r)
```

### Observations:

 - Critical causes very long run time. Because each in each iteration only one thread at a time can enter the critical section, therefore the thread count increases the run time. This is against the pthread results.
 
 - Atomic version increases slightly with more threads.
 
 - Reduction version is very fast, even faster then the pthread version.



## Task 5: Benchmarking differtent OpenMP versions with /usr/bin/time 
```{r OMP Benchmark usr/bin/time user time, echo=FALSE}
data<-data.frame(
  legend = factor(rep(c("critcal", "atomic", "reduction"), each=5)),
  num_threads=rep(c(1,2,4,6,12), times=3),
  measurements = c(14.13, 84.13, 410.513, 867.37, 1968.06, # critical
                    11.29, 23.07, 83.193, 177.46, 261.43, # atomic
                    8.38, 8.46, 8.476, 8.75, 8.91 # reduction
                   ) 
)

s <- ggplot(data, aes(x=num_threads, y=measurements, color= legend, group=legend)) +
  ggtitle("Benchmark OMP versions, /usr/bin/time user time")+
  geom_line() +
  geom_point() +
  xlab("number threads") +
  ylab("Execution time in seconds")+
  scale_x_continuous(limits = c(1,12), breaks = c(1,2,4,6,12))
show(s)
```

```{r OMP-Critical Benchmark wall-time, echo=FALSE}
data<-data.frame(
  legend = factor(rep(c("critcal", "atomic", "reduction"), each=5)),
  num_threads=rep(c(1,2,4,6,12), times=3),
  measurements = c( 14.16, 42.48, 105.063, 148.95, 174.05,# critical
                    11.31, 11.56, 21.67, 30.22, 22.32,# atomic
                     8.40, 4.25, 2.13, 1.465, 0.79# reduction
                   ) 
)

t <- ggplot(data, aes(x=num_threads, y=measurements, color= legend, group=legend)) +
  ggtitle("Benchmark OMP versions, /usr/bin/time wall time")+
  geom_line() +
  geom_point() +
  xlab("number threads") +
  ylab("Execution time in seconds")+
  scale_x_continuous(limits = c(1,12), breaks = c(1,2,4,6,12))
show(t)
```

### Observations:
`/usr/bin/time` wall time is the same as the wtime measurements of OpenMP. Whereas the `/usr/bin/time` user time sums up the execution time of all involved threads.

# Exercise 2
## Task 3
We found the ideal padding size to be 64 Bytes. We found this by executing this command: `getconf -a | grep CACHE` and look at the cache line size value.

## Task 4
```{r False sharing execution time, echo=FALSE}
data<-data.frame(
  legend = factor(rep(c("private variable", "array", "array with padding"), each=5)),
  num_threads=rep(c(1,2,4,6,12), times=3),
  measurements = c( 8.424, 4.253, 2.153, 1.470, 0.535,# private variable
                    10.987, 7.779, 10.616, 11.278, 8.490,# array
                    10.959, 5.528, 2.786, 1.955, 1.019# array with padding
                   ) 
)

u <- ggplot(data, aes(x=num_threads, y=measurements, color= legend, group=legend)) +
  ggtitle("False sharing execution time")+
  geom_line() +
  geom_point() +
  xlab("number threads") +
  ylab("Execution time in seconds")+
  scale_x_continuous(limits = c(1,12), breaks = c(1,2,4,6,12))
show(u)
```

```{r False sharing cache misses, echo=FALSE}
data<-data.frame(
  legend = factor(rep(c("private variable", "array", "array with padding"), each=5)),
  num_threads=rep(c(1,2,4,6,12), times=3),
  measurements = c( 10681, 11418, 15.231, 14041, 18181,# private variable
                    9283, 151234630, 291608587, 306297540, 261108315,# array
                    9258, 11080, 11696, 17537, 34638 # array with padding
                   ) 
)

v <- ggplot(data, aes(x=num_threads, y=measurements, color= legend, group=legend)) +
  ggtitle("False sharing cache misses")+
  geom_line() +
  geom_point() +
  xlab("number threads") +
  ylab("L1 D-Cache load misses")+
  scale_x_continuous(limits = c(1,12), breaks = c(1,2,4,6,12))+
  scale_y_continuous(name = "#Cachemisses in log10", trans="log10")
show(v)
```

### Observations:
The version with an array without padding has terrible cache miss count. The other two version have very similar cache miss counts, with private version slightly better.
In terms of execution times private variable is best with padding being slightly slower. Both of those improve with more threads, whereas the normal array version get slower with more threads.
